{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad87508a",
   "metadata": {},
   "source": [
    "# Week 3: The data science ecosystem\n",
    "\n",
    "<img width = \"250\" src=\"./images/hazel.png\" align=\"right\" style=\"padding-left:10px\">\n",
    "\n",
    "Congrats. You have learned the foundations of Python! So far, we have studied the Python standard library, but have not looked at all at the external libraries that people typically use for data analysis.\n",
    "\n",
    "This week you will start this journey, going through a crash course on the most essential tools you will need to analyze and visualize data. Since we are done with ATBS, it is time to delegate our flipped teaching responsibilities to multiple sources. I basically scoured the internet for readable, beginner-friendly, but practical introductions to the topics we are covering. I think I have found some really good resources, but if you find any additional material that you like, please let me know!\n",
    "\n",
    "- [1: Overview of Python's data science ecosystem](#intro)\n",
    "- [2: Virtual Environments](#virtual)\n",
    "- [3: Numpy for numerical computing](#numpy)\n",
    "- [4: Matplotlib for plotting](#matplotlib)\n",
    "- [5: Pandas for analysis of tabular data](#pandas)\n",
    "\n",
    "Note each of these libraries could fill multiple weeks of discussion. Our goal isn't to become an expert in matplotlib or pandas. Rather, I want you to become comfortable running basic commands, loading the libraries, and especially creating your own *analysis environments* in Python that you can quickly build up. This will be crucial for larger-scale analysis projects, like the one we will tackle in the final week with Deep Lab Cut.  \n",
    "\n",
    "Incidentally, there is a good book on Python for data science that is free and available online: [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/). Unfortunatley, the material in that book is *way* too dense for a rapid crash course. However, when you get stuck or have questions, I encourage you to use it as a reference (along with Google).\n",
    "\n",
    "My recommendation for picking up the material this week is the same as before. There is really no major break in method when learning to use the in the standard library and tools in third-party libraries:  make lots of cells and mess around with code. Tweak and twiddle with the examples in the videos and web pages to see what happens. Coding is largely muscle memory, so it is really important to literally *type code* rather than just read and understand the tutorials. If you run into error messages, first just inspect your code to make sure there isn't an obvious mistake (an unclosed paren or quotation mark). Second, read the error message: they are sometimes helpful. Barring that, Google the error message there are great resources online. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91e306",
   "metadata": {},
   "source": [
    "<a id =\"intro\"></a>\n",
    "# 1: Overview of Python's data science ecosystem\n",
    "You will often hear about Python *data science stack*, which is the main set of Python libraries that people use to analyze and visualize their data. It includes libraries for numerical computing, plotting, statistical tests, and much more. If you want to analyze data, there is a core set of libraries to be familiar with. This does not mean you need to be an expert: it is more about being comfortable enough to install and use the library's basic functionality (one secret with these libraries is just a handful of commands is enough). \n",
    "\n",
    "The data science stack isn't some crisply defined list defined by some committee of developers. Indeed, *it really isn't a stack at all*. While I will write/talk about the data science stack, this metaphor implies some foundation where one piece builds on another. The data analysis tools form more of an **ecosystem**, an interconnected web of software packages all geared toward helping you analyze your data. Here is a cross-section through some of the resources available in Python:\n",
    "\n",
    "<img width = \"650\" src=\"./images/ds_ecosystem.jpg\" style=\"padding-top:30px; padding-bottom:30px\">\n",
    "\n",
    "As you can see there are a few core packages in the center (e.g., Numpy): these are the packages with which most people are familiar, and that many other packages depend upon. Then there are more peripheral packages located more at the edges of the web. These are the ones that you will pick up when you have a particular need (e.g., you don't need to learn Tensorflow unless you are doing a particular type of machine learning project). The more peripheral packages aren't lower quality, they are just *specialized* for particular tasks that many people will never need. All the packages are typically very high quality, that's how they make it into the ecosystem.\n",
    "\n",
    "I can't stress enough: you **do not need** to learn all these packages. That would be a waste of time, especially when starting out. My plan is to help you navigate this web, cut through the complexity, and find your bearings. More specifically, our goal is threefold (**to do**: shouldn't this be above, not here in the middle of the section in which it is currently happening?):\n",
    "- Informally discuss the main packages in the data science ecosystem. That way, in the future, you can have an idea what tool to use when you have a specific need. I do not recommend that you pre-emptively learn a package that you *might* need someday, unless it is serving some useful pedagogical purpose.\n",
    "- Discuss and create *virtual environments*, which are extremely important in any serious analysis project (and you will use them in the final two weeks).\n",
    "- Learn in more detail about three important tools in the data science ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d0b42",
   "metadata": {},
   "source": [
    " ## Synopsis of specific packages\n",
    "Before jumping into learning more specifics, let's do a quick overview of the main packages in the ecosystem.\n",
    "\n",
    "- **numpy**: NumPy is a numerical computing library that supports extremely efficient array computations (as we will see, arrays are basically containers for numbers and are extremely useful for representing data). It also has basic mathematical operations and statistical calculations on those numerical arrays. NumPy forms the basis for almost all packages that perform operations on numbers.\n",
    "- **matplotlib**: Matlplotlib is the core plotting library for Python. \n",
    "- **scipy**: Scipy is a scientific computing package that is basically one abstraction level up from NumPy. It includes basic statistics, optimization, numerical integration,  and other scientific computing tools. It is a sort of grab-bag for useful computations that you will use across multiple projects. \n",
    "\n",
    "The above tools are sort of the core of the Python data science ecosystem. If you do any heavy data analysis, I can pretty much *guarantee* you will use the above libraries. The other libraries typically build on the above: I would consider them more peripheral, in the sense that you may or may not use them, depending on your specialization and the type of data you use. They include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6f657",
   "metadata": {},
   "source": [
    "- **Pandas**: an extremely user-friendly package for handling tabular data (basically, data arranged in something like an Excel spreadsheet). Pandas includes tools for analysis as well as plotting such data. It builds extensively on numpy and Matplotlib. Below, we will learn the basics of Pandas, partly because it is used extensively in Neuroscience, and because a lot of data in neuroscience is stored in the `csv` format.\n",
    "- **Pillow**: A simple image processing library (initially, there was PIL (Python Image Library) and Pillow is based on it). Allows for reading/writing images, converting between image types, and applying simple filters (e.g., contrast/smoothing) to images. \n",
    "- **OpenCv**: An extremely powerful computer vision library. Written in C++, a Python wrapper lets you use it using pure Python. It allows you to capture video from your web camera, read and write movies, and has tools for extensive processing and filtering of your images and movies, including machine vision in real time (e.g., facial recognition). \n",
    "- **scikit-learn**: The main machine learning library when you want to do traditional (not-neural network based) machine learning. It has tools for classification, clustering, and regression. It has a consistent interface for these problems, and is fairly intuitive to use.\n",
    "- **tensorflow/pytorch**: These are the two main *deep learning* packages that use artificial neural networks to let you tap into the power of cutting edge machine learning algorithms. Tensorflow was developed by Google and has been around longer, while Pytorch was developed by Facebook and is a bit easier to use and learn. Such approaches tend to work great for complex machine vision problems, but are often not necessary (i.e., scikit-learn is often more than enough). \n",
    "- **Plotly / Bokeh / Seaborn (etc)**: while Matplotlib is a great plotting library, there are some limitations. Because of this, there has been [a proliferation of plotting libraries in Python](https://geo-python-site.readthedocs.io/en/stable/lessons/L7/python-plotting.html). I've just listed some of the more popular ones here. *Seaborn* is built on Matplotlib and provides an intuitive interface to generate beautiful plots even with complex data, plots that would take many many lines of code using Matplotlib.  *Plotly* and *Bokeh* allow you to embed beautiful interactive plots in Jupyter notebooks: they are basically Python interfaces with a Javascript (web-page friendly) back end. Plotly and Bokeh can also be integrated into dashboards that you can share online, or even in the cloud. \n",
    "- **sympy**: the symbolic mathematics library for Python. This is not used that much by people tussling with real messy data. However, if you need an *exact* solution to an equation, then you can use `sympy`. \n",
    "- **statsmodels**: for specialized or advanced statistical analysis: if SciPy doesn't have what you need, check statsmodels. This used to be part of SciPy, but branched off into its own specialized package.\n",
    "- **PyMC3**: A tool specialized for Bayesian modeling and inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c39b5",
   "metadata": {},
   "source": [
    "## How to navigate the ecosystem\n",
    "The above is a guided tour of the main components of the Python data science ecosystem. Note do *not* worry about memorizing everything in there: I'd just try to remember the general types of tools available. Then when you need one you know you will be able to find it. \n",
    "\n",
    "Honestly, there is a good chance you will *never* use some of the libraries in the secondary stack, especially if you are not a full-time programmer. I *am* a full-time programmer and I have never explicitly imported `PyMC3` or `sympy`. They are perfectly good packages, I just haven't needed them yet.  \n",
    "\n",
    "Below, we will focus on learning the basics of just three of the packages in the data science stack. This will give you a sense for how to install, import, and work with different libraries. Once you've done it a couple of times, your skill will generalize to new libraries when they are needed.\n",
    "\n",
    "In general with Python, for important tasks (e.g., web scraping) there will typically be a canonical framework out there that does it really well. Finding it is typically easy with Google. There are also curated lists of great Python packages: my favorite is the [awesome python](https://github.com/vinta/awesome-python) list, which is fairly comprehensive and organized by general category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745f0d3",
   "metadata": {},
   "source": [
    "### What about Jupyter, github, and IDEs?\n",
    "While Jupyter notebooks aren't part of your first-order analysis workflow (i.e., you don't ever *import* Jupyter in your code), they have become the de-facto scaffolding for workflows, so I thought it was important to include it in the figure above. \n",
    "\n",
    "With their extremely convenient web-interface for coding, they have become absorbed into all the major cloud computing platforms (AWS, Google Colabs, and Azure). Further, Jupyter notebooks are the tool of choice for book authors, and creators of new libraries that want to get people on board quickly. It used to be you had to worry about what IDE someone was using when you shared code -- now you know you can just provide a Jupyter notebook, and things will be fine. \n",
    "\n",
    "If this class were for software developers, **git** and **github** would also be in the diagram. By making it extremely easy to track and share code, such tools have been crucial for speeding up development of analysis software, and removing arbitrariness from the process. Since this class is for neuroscientists, who often will not use a version control system, I decided to leave it out. \n",
    "\n",
    "Similarly, as discussed in `week0.ipynb`,  *integrated development environments* (IDEs) are extremely important components of any programmer's workflow: Jupyter notebooks simply cannot compare in terms of power/speed/efficiency for writing and refactoring code. Jupyter notebooks have been a boon for *communicating* code. Our purpose here is more didactic, and Jupyter notebooks are often sufficient for most analysis purposes: if you ever feel the need to go beyond the notebook, to something more powerful, then there are plenty available (see Section 2 of `week0.ipynb` where we went over some options)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144b8cb",
   "metadata": {},
   "source": [
    "<a id =\"virtual\"></a>\n",
    "<img width = \"250\" src=\"./images/sandbox.png\" align=\"right\" style=\"padding-left:10px\">\n",
    "# 2: Virtual environments: your Python sandbox\n",
    "\n",
    "Before we start with numpy, we will to learn about *virtual environments*. This will be a brief tour of anaconda virtual environments. We'll start with an introduction and some background material before building our own virtual environment.\n",
    "\n",
    "## Virtual environments: what and why?\n",
    "In the final two classes we will go beyond the standard Python library, forging fairly deeply into the data science jungle. Hence, we need to start thinking about how to  manage the contexts in which we carry out our work in Python. We need to talk about *virtual environments*.\n",
    "\n",
    "A virtual environment is an isolated playground or sandbox made up of a Python version and a set of software packages. You typically construct a virtual environment to carry out a particular task. That task might be extremely general (\"Practice doing things in the data science ecosystem\") or specific (\"Set up deep lab cut and run it on my fly data\"). Each virtual environment will have its own unique name and can easily be activated within conda at the command line. \n",
    "\n",
    "Why do we need virtual environments?\n",
    "\n",
    "<img width=\"180\" src=\"./images/virt_env_thumbsup.jpg\" align=\"right\" style=\"padding-left:10px\">\n",
    "\n",
    "As we work on different projects, we will find that they frequently depend on different, often incompatible combinations of software packages. For instance, you might have one project that uses Python 3.6 and Tensorflow v1. Another project may use Python  3.8 and Tensorflow v2. It would be agonizing to have to build up and tear down such environments each time you need to use a new one on the same machine. \n",
    "\n",
    "Wouldn't it be easier to build up temporary isolated *virtual* environments that you could activate and work within, on an as needed basis, for these different projects?  That way, you wouldn't have to worry about making breaking changes to your carefully constructed architecture every time you wanted to switch projects.\n",
    "\n",
    "Also, what if, you have a carefully constructed combination of software packages that work really well for a particular analysis pipeline (maybe a pipeline used in a paper)? Wouldn't it be nice if you could share this, such that someone could easily reconstruct it on their own computer?  This would not only make things convenient, but make for more *reproducible science*!\n",
    "\n",
    "Virtual environments let you do all the above things in an intuitive and easy way, without having to send a bunch of error-prone step-by-step installation instructions. Also, since you have been using Anaconda this whole time, it will be very easy to get it working. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c601d1",
   "metadata": {},
   "source": [
    "## Virtual environments in conda\n",
    "In fact, while we haven't discussed it, you have been working within a virtual environment in conda this whole time: you have been working from your anaconda `base` environment! To see this, open up your conda prompt (whatever command line you use to open Jupyter notebooks). You should see the word `base` in parentheses before the command line prompt. This means you are in your base anaconda environment. \n",
    "\n",
    "We can see more details about conda virtual environments in the following image, created by User Interface Designer [Kristztina Szerovay](https://krisztina.szerovay.hu/):\n",
    "\n",
    "<img width = \"700\" src=\"./images/virtual_environments.jpg\">\n",
    "\n",
    "This figure deserves close study. At the top, we have set up the conda package manager which handles *everything*. You installed this (in the form of miniconda) in the first class. On the lower left is the `base` environment (called the `root` environment in the figure) which is the default environment that contains the Python standard library (you can also install any packages you want available in your base environment). You have been working in your base environment since Day 1.\n",
    "\n",
    "On the right hand side are the additional environments. Each environment contains a version of Python that can be different from that in the base, and software packages that are molded to the environment's purpose. The name of each environment should be something that clearly reminds you of its purpose, but it can be anything you like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0196c5",
   "metadata": {},
   "source": [
    "## Creating and activating your first environment\n",
    "The above discussion is fairly abstract and theoretical. Let's get some practical experience and see how all this works. Creating new environments is really easy in conda. In your anaconda prompt, just use the command:\n",
    "\n",
    "    conda create -n <env_name> \n",
    "    \n",
    "Where `<env_name>` is the name of the environment you want to create. I try to pick environment names so that it is pretty clear what that environment is for. For this class, let's create a data science virtual environment:\n",
    "\n",
    "    conda create -n datasci\n",
    "    \n",
    "You will see a bunch of stuff, and be asked if you want to proceed, enter `y` and hit your ENTER key, and you now have your virtual environment!  \n",
    "\n",
    "Now we are ready to play, right? **Not yet**. You must first *activate* the environment: \n",
    "\n",
    "    conda activate datasci\n",
    "\n",
    "Now you will notice that the name in parentheses before your command line prompt has switched from `(base)` to `(datasci)`:\n",
    "\n",
    "<img width = \"400\" src=\"./images/conda_venv_activate.jpg\">\n",
    "\n",
    "In general, this is how you know what virtual environment you are in: conda wants this to be very easy to keep track of!\n",
    "\n",
    "I think the most common mistake people make when starting out with virtual environments is forgetting to activate an environment, and then wondering \"What is wrong with my software?\". I think this is so common it should basically be elevated to a rite of passage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b875be",
   "metadata": {},
   "source": [
    "## Building up your environment\n",
    "Once inside of our activate `datasci` environment, it isn't quite time to play yet. Right now it is just an empty placeholder with a name: we first have to *build* the environment by installing the packages that we want. \n",
    "\n",
    "Let's install four packages in our environment: jupyter, numpy, matplotlib, and pandas. At your conda prompt, enter the following commands:\n",
    "\n",
    "    conda activate datasci # just in case you didn't already do this\n",
    "    conda install -c conda-forge numpy matplotlib pandas notebook\n",
    " \n",
    "You will be prompted to confirm you want to install these libraries (and their many, many dependencies). Enter 'y' and sit back for a few minutes while conda installs everything.\n",
    "\n",
    "> Note: the `-c conda-forge` part of the install command is telling conda to install these packages from the `conda-forge` channel, which is where software developers maintain the installers for their software packages. \n",
    "\n",
    "Once you've done the above, you've successfully finished building your datasci environment! You can now enter `jupyter notebook` at the command line prompt as you have been doing all along, and your Jupyter server will start. The cool thing is that now the notebook will start *within the context of your new virtual environment*, so you will be able to import numpy, matplotlib, and pandas in your Jupyter notebook. In your base environment, if you try to import those packages, you will get an error because they have not been installed.  \n",
    "\n",
    "Congratulations, you've just created and built up your first virtual environment! This is a very important step. If you ever take on independent projects of any complexity, you will want to consider working on them in a new virtual environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d047e",
   "metadata": {},
   "source": [
    "## Deactivating environments, and a few loose ends\n",
    "What about when you are done working within a virtual environment? When you are ready to *deactivate* an environment and go back to your base environment, just enter:\n",
    "\n",
    "    conda deactivate\n",
    "    \n",
    "And your environment will go back to base.\n",
    "\n",
    "To see what packages you have installed in your virtual environment, use the following:\n",
    "\n",
    "    conda list\n",
    "\n",
    "To see a list of virtual environments you have in your system:\n",
    "\n",
    "    conda env list\n",
    "    \n",
    "When you run this, you should now have `base` and `datasci` environment (as well as any others you might have created in the past).\n",
    "\n",
    "Finally, what if your virtual environment is fubar and you want to delete it? Maybe you are getting errors that you can't fix, and are ready to just start over and install things in a different order. To remove a virtual environment from your computer, use the following command:\n",
    "\n",
    "    conda remove -n <env_name> --all\n",
    "    \n",
    "Just be sure you really want to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbaa87",
   "metadata": {},
   "source": [
    "## To learn more \n",
    "Virtual environments is a big topic, and there are lots of resources, but you really don't need to know much more than the above to get started in practical terms. \n",
    "\n",
    "If you do want to learn about conda virtual environments in more depth (like how to save/share an environment), see the following):\n",
    "- https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n",
    "- https://www.freecodecamp.org/news/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c/\n",
    "\n",
    "\n",
    "Our main goal now is to learn by doing, so let's put our `datasci` virtual environment to work by learning some numpy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da37cae",
   "metadata": {},
   "source": [
    "<a id =\"numpy\"></a>\n",
    "# 3: Numpy for numerical computing\n",
    "\n",
    "Imports\n",
    "Youâ€™ve already done this (mention parts in ATBS)\n",
    "The following is a really good summary of imports.\n",
    "https://medium.com/cold-brew-code/a-quick-guide-to-understanding-pythons-import-statement-505eea2d601f \n",
    "\n",
    "Other references:\n",
    "https://www.digitalocean.com/community/tutorials/how-to-import-modules-in-python-3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd473d",
   "metadata": {},
   "source": [
    "<a id =\"matplotlib\"></a>\n",
    "# 4: Plotting with matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dc827",
   "metadata": {},
   "source": [
    "<a id =\"matplotlib\"></a>\n",
    "# 5: Pandas for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83462149",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    <h1>Congratulations!!!</h1>\n",
    "</span>\n",
    "<img width = \"150\" src=\"./images/yippee.jpg\" align=\"left\" style=\"padding-right:10px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Further reading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
